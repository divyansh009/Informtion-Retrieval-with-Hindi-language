# -*- coding: utf-8 -*-
"""q3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xcWUnjOf8lHk93ZSGOrmVeCO4geV_c2V
"""

#First we will be importing the dataset as a tar file
!wget -cq https://storage.googleapis.com/ai4bharat-public-indic-nlp-corpora/indiccorp/hi.tar.xz

#Now we'll be expanding tar now
import tarfile #Importing the file
tar=tarfile.open("hi.tar.xz")
tar.extractall()
tar.close()

!rm -r hi.tar.xz #To delete tar files as they are not of any use now

# Setup
!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git
!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git
!pip install indic-nlp-library
import sys
from indicnlp import common
# The path to the local git repo for Indic NLP library
INDIC_NLP_LIB_HOME=r"indic_nlp_library"
# The path to the local git repo for Indic NLP Resources
INDIC_NLP_RESOURCES=r"indic_nlp_resources"
# Add library to Python path
sys.path.append(r'{}\src'.format(INDIC_NLP_LIB_HOME))
# Set environment variable for resources folder
common.set_resources_path(INDIC_NLP_RESOURCES)

from collections import Counter

#Now, we'll carry out the word bigram process
from indicnlp.tokenize.indic_tokenize import trivial_tokenize
from indicnlp.normalize.indic_normalize import IndicNormalizerFactory
lang = 'hi'
tokendict2 = {}
normalizer_factory = IndicNormalizerFactory()
normalizer = normalizer_factory.get_normalizer(lang)
linecount = 0
with open("data/hi/hi.txt", 'r', encoding='utf-8') as in_fp:
    for line in in_fp:
        linecount += 1
        sent = line.rstrip('\n')
        normalized = normalizer.normalize(sent)
        processed = ' '.join(trivial_tokenize(normalized, lang))
        # tokendict += Counter(processed.split())
        words_in_sentence = processed.split()
        for i in range(1, len(words_in_sentence)):
            ptok = words_in_sentence[i-1] + ' ' + words_in_sentence[i]
            if ptok in tokendict2.keys():
                tokendict2[ptok] += 1
            else:
                tokendict2[ptok] = 1
        if(linecount == 1000000):
            # print(linecount)
            tempdict = {key:val for key, val in tokendict2.items() if val >= 100}
            tokendict2 = tempdict
            linecount = 0

c=Counter(tokendict2)
c.most_common(100)

"""##Q3 (b) - words"""

# Uni-gram words
# Loading dictionary object for word-frequency from pickle file generated earlier 
import pickle
 
with open('/content/drive/My Drive/word_unigram_unsorted', 'rb') as word_unigram_unsorted_file:
    word_unigram_dict = pickle.load(word_unigram_unsorted_file)

# To filter punctuations and other symbols
tempdict = {key:val for key, val in word_unigram_dict.items() if key not in ['।',',','.','-','"',"'",'(',')',':']}
word_unigram_dict = tempdict

# Considering keys with significance
tempdict = {key:val for key, val in word_unigram_dict.items() if val > 100}
word_unigram_dict = tempdict

# subclass of dict for most common method
from collections import Counter
c = Counter(word_unigram_dict)
c = c.most_common(100)
c

import matplotlib.pyplot as plt
import numpy as np
y = []
for i in c:
    y.append(i[1])
ypoints = np.array(y)
plt.plot(ypoints)
plt.title("Words uni-gram frequency")
plt.xlabel("Most freq words in desc")
plt.ylabel("Normalized frequency")
plt.show()

# Bi-gram words
with open('/content/drive/My Drive/word_bigram_unsorted', 'rb') as word_bigram_unsorted_file:
    word_bigram_dict = pickle.load(word_bigram_unsorted_file)

tempdict = {key:val for key, val in word_bigram_dict.items() if key.find('।')==-1 and key.find('.')==-1 and key.find(',')==-1}
# not in ['।',',','.','-','"',"'",'(',')',':']
word_bigram_dict = tempdict

c = Counter(word_bigram_dict)
c = c.most_common(100)
c

y = []
for i in c:
    y.append(i[1])
ypoints = np.array(y)
plt.plot(ypoints)
plt.title("Words bi-gram frequency")
plt.xlabel("Most freq bi-gram words in desc")
plt.ylabel("Normalized frequency")
plt.show()

# Tri-gram words
with open('/content/drive/My Drive/word_trigram_unsorted', 'rb') as word_trigram_unsorted_file:
    word_trigram_dict = pickle.load(word_trigram_unsorted_file)

tempdict = {key:val for key, val in word_trigram_dict.items() if key.find('।')==-1 and key.find('.')==-1 and key.find(',')==-1 and key.find('-')==-1  and key.find('(')==-1  and key.find(')')==-1 }
# not in ['।',',','.','-','"',"'",'(',')',':']
word_trigram_dict = tempdict

c = Counter(word_trigram_dict)
c = c.most_common(100)
c

y = []
for i in c:
    y.append(i[1])
ypoints = np.array(y)
plt.plot(ypoints)
plt.title("Words tri-gram frequency")
plt.xlabel("Most freq tri-gram words in desc")
plt.ylabel("Normalized frequency")
plt.show()

"""##Q3 (a) - characters"""

# Method to fix halant character for every word. 
# Consonants now considered as halant representations followed by vowels
def fix_halank(word):
    consonants=['क','ख','ग','घ','ड़','च','छ','ज','झ','ञ','ट','ठ','ड','ढ','ण', 'त','थ','द','ध','न','प','फ','ब','भ','म','य','र','ल','व', 'श','ष','स','ह']
    wordlist=[]
    wordlist.append(word[0])
    for i in range(1, len(word)):   
        if (word[i] in consonants) and (word[i-1] in consonants):
            wordlist.append('अ')
        wordlist.append(word[i])
    if word[-1] in consonants:
        wordlist.append('अ')
    wordlist2 = ([s for s in wordlist if s != '्'])
    word = ''.join(wordlist2)
    return(word)

# UNIGRAM CHAR
lang='hi'
unichardict = {}
for key, val in word_unigram_dict.items():
    word = fix_halank(key)
    # syllables = ' '.join(syllabifier.orthographic_syllabify(key, lang))
    for ch in word:
        if ch in unichardict.keys():
            unichardict[ch] += val
        else:
            unichardict[ch] = val

tempdict = {key:val for key, val in unichardict.items() if not key.isnumeric()}
unichardict = tempdict

c = Counter(unichardict)
c = c.most_common(100)
c

import matplotlib.pyplot as plt
import numpy as np
y = []
for i in c:
    y.append(i[1])
ypoints = np.array(y)
plt.plot(ypoints)
plt.title("Char uni-gram frequency")
plt.xlabel("Most freq chars in desc")
plt.ylabel("Normalized frequency")
plt.show()

# BIGRAM CHAR
lang='hi'
bichardict = {}
for key, val in word_unigram_dict.items():
    word = fix_halank(key)
    for i in range(1, len(word)):
        ch = word[i-1] + word[i]
        if ch in bichardict.keys():
            bichardict[ch] += val
        else:
            bichardict[ch] = val

c = Counter(bichardict)
c = c.most_common(100)
c

y = []
for i in c:
    y.append(i[1])
ypoints = np.array(y)
plt.plot(ypoints)
plt.title("Chars bi-gram frequency")
plt.xlabel("Most freq bi-gram chars in desc")
plt.ylabel("Normalized frequency")
plt.show()

# TRIGRAM CHAR
lang='hi'
trichardict = {}
for key, val in word_unigram_dict.items():
    word = fix_halank(key)
    for i in range(2, len(word)):
        ch = word[i-2] + word[i-1] + word[i]
        if ch in trichardict.keys():
            trichardict[ch] += val
        else:
            trichardict[ch] = val

c = Counter(trichardict)
c = c.most_common(100)
c

y = []
for i in c:
    y.append(i[1])
ypoints = np.array(y)
plt.plot(ypoints)
plt.title("Chars tri-gram frequency")
plt.xlabel("Most freq tri-gram chars in desc")
plt.ylabel("Normalized frequency")
plt.show()

# QUADGRAM CHAR
lang='hi'
quadchardict = {}
for key, val in word_unigram_dict.items():
    word = fix_halank(key)
    for i in range(3, len(word)):
        ch = word[i-3] + word[i-2] + word[i-1] + word[i]
        if ch in quadchardict.keys():
            quadchardict[ch] += val
        else:
            quadchardict[ch] = val

c = Counter(quadchardict)
c = c.most_common(100)
c

y = []
for i in c:
    y.append(i[1])
ypoints = np.array(y)
plt.plot(ypoints)
plt.title("Chars quad-gram frequency")
plt.xlabel("Most freq quad-gram chars in desc")
plt.ylabel("Normalized frequency")
plt.show()

"""##Q3 (c) - syllables"""

def syllables_composition(word):
    word = fix_halank(word)
    syllables = []
    syll = []
    vowels=['अ','आ','इ','ई','उ','ऊ','ऋ','ए','ऐ','ओ','औ','अं','अः','ऻ','ि','ी','ू','ृ','ॄ','ॅ','ॆ','े', 'ं', 'ै','ॉ','ॊ','ो','ौ','्','ॎ','ॏ','ु']
    for ch in word:
        if ch in vowels:
            syll.append(ch)
            syllables.append(''.join(syll))
            syll = []
        else:
            syll.append(ch)
    return syllables

print(syllables_composition('जगदीशचंद्र'))

# UNIGRAM SYLLABLE
unisylldict = {}

for key, val in word_unigram_dict.items():
    syllables = syllables_composition(key)
    for i in range(len(syllables)):
        syll_combo = syllables[i]
        if syll_combo in unisylldict.keys():
            unisylldict[syll_combo] += val
        else:
            unisylldict[syll_combo] = val

c = Counter(unisylldict)
c = c.most_common(100)
c

y = []
for i in c:
    y.append(i[1])
ypoints = np.array(y)
plt.plot(ypoints)
plt.title("Syllables uni-gram frequency")
plt.xlabel("Most freq syllables in desc")
plt.ylabel("Normalized frequency")
plt.show()

# BIGRAM SYLLABLE
bisylldict = {}
for key, val in word_unigram_dict.items():
    syllables = syllables_composition(key)
    for i in range(1, len(syllables)):
        syll_combo = syllables[i-1] + syllables[i]
        if syll_combo in bisylldict.keys():
            bisylldict[syll_combo] += val
        else:
            bisylldict[syll_combo] = val

c = Counter(bisylldict)
c = c.most_common(100)
c

y = []
for i in c:
    y.append(i[1])
ypoints = np.array(y)
plt.plot(ypoints)
plt.title("Syllables bi-gram frequency")
plt.xlabel("Most freq bi-gram syllables in desc")
plt.ylabel("Normalized frequency")
plt.show()

# TRIGRAM SYLLABLE
trisylldict = {}
for key, val in word_unigram_dict.items():
    syllables = syllables_composition(key)
    for i in range(2, len(syllables)):
        syll_combo = syllables[i-2] + syllables[i-1] + syllables[i]
        if syll_combo in trisylldict.keys():
            trisylldict[syll_combo] += val
        else:
            trisylldict[syll_combo] = val

c = Counter(trisylldict)
c = c.most_common(100)
c

import matplotlib.pyplot as plt
import numpy as np
y = []
for i in c:
    y.append(i[1])
ypoints = np.array(y)
plt.plot(ypoints)
plt.title("Syllables tri-gram frequency")
plt.xlabel("Most freq tri-gram syllables in desc")
plt.ylabel("Normalized frequency")
plt.show()